{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors = [\n",
    "    {\n",
    "        \"vendor_id\": \"V001\",\n",
    "        \"name\": \"Rapid Plumbers\",\n",
    "        \"category_expertise\": [\"repairs\", \"plumbing\"],\n",
    "        \"response_time_hours\": 1,\n",
    "        \"preferred_communication_style\": \"casual\",\n",
    "        \"past_reliability_score\": 4.8,\n",
    "        \"constraints\": [\"Only serves downtown area\", \"Requires 2hr lead time for non-urgent tasks\"]\n",
    "    },\n",
    "    {\n",
    "        \"vendor_id\": \"V002\",\n",
    "        \"name\": \"City Cabs Express\",\n",
    "        \"category_expertise\": [\"travel\", \"transport\"],\n",
    "        \"response_time_hours\": 0.25, # 15 minutes\n",
    "        \"preferred_communication_style\": \"formal\",\n",
    "        \"past_reliability_score\": 4.5,\n",
    "        \"constraints\": [\"No airport pickups after midnight\"]\n",
    "    },\n",
    "    {\n",
    "        \"vendor_id\": \"V003\",\n",
    "        \"name\": \"Handy Helpers\",\n",
    "        \"category_expertise\": [\"errands\", \"repairs\", \"gardening\"],\n",
    "        \"response_time_hours\": 4,\n",
    "        \"preferred_communication_style\": \"casual\",\n",
    "        \"past_reliability_score\": 4.2,\n",
    "        \"constraints\": [\"Only accepts tasks with >12h lead time\"] # [cite: 5]\n",
    "    },\n",
    "    {\n",
    "        \"vendor_id\": \"V004\",\n",
    "        \"name\": \"Formal Fleet\",\n",
    "        \"category_expertise\": [\"travel\", \"corporate transport\"],\n",
    "        \"response_time_hours\": 2,\n",
    "        \"preferred_communication_style\": \"formal\",\n",
    "        \"past_reliability_score\": 4.9,\n",
    "        \"constraints\": [\"Minimum booking duration 2 hours\"]\n",
    "    },\n",
    "     {\n",
    "        \"vendor_id\": \"V005\",\n",
    "        \"name\": \"Expert Electricians\",\n",
    "        \"category_expertise\": [\"repairs\", \"electrical\"],\n",
    "        \"response_time_hours\": 3,\n",
    "        \"preferred_communication_style\": \"formal\",\n",
    "        \"past_reliability_score\": 4.6,\n",
    "        \"constraints\": [\"Requires clear description of issue beforehand\", \"Does not work on weekends\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {\n",
    "        \"task_id\": \"T101\",\n",
    "        \"task_description\": \"I need a plumber urgently for a leaking tap in the kitchen.\", # [cite: 5]\n",
    "        \"category\": \"plumbing\",\n",
    "        \"urgency\": \"high\",\n",
    "        \"special_requirements\": \"Need someone who can bring spare parts for standard faucet types.\" # [cite: 5]\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"T102\",\n",
    "        \"task_description\": \"Book a cab for 2 people to the main railway station for tomorrow morning at 8:00 AM.\",\n",
    "        \"category\": \"travel\",\n",
    "        \"urgency\": \"medium\",\n",
    "        \"special_requirements\": \"Need space for two large suitcases.\"\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"T103\",\n",
    "        \"task_description\": \"Pick up groceries from the list I will provide via email. Delivery needed by 6 PM today.\",\n",
    "        \"category\": \"errands\",\n",
    "        \"urgency\": \"medium\",\n",
    "        \"special_requirements\": \"Please ensure fragile items like eggs are handled carefully.\"\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"T104\",\n",
    "        \"task_description\": \"Need a reliable car service for a corporate client pickup from the airport next Monday at 10 AM.\",\n",
    "        \"category\": \"travel\",\n",
    "        \"urgency\": \"low\",\n",
    "        \"special_requirements\": \"Driver must be formally dressed, sedan car preferred.\"\n",
    "    },\n",
    "    {\n",
    "        \"task_id\": \"T105\",\n",
    "        \"task_description\": \"My power outlet in the living room is sparking. Need an electrician ASAP.\",\n",
    "        \"category\": \"electrical\",\n",
    "        \"urgency\": \"high\",\n",
    "        \"special_requirements\": \"Safety is paramount, please send a certified professional.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict, Optional\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vendors_by_category(vendors, task):\n",
    "    task_category = task.get(\"category\")\n",
    "    if not task_category:\n",
    "        return vendors\n",
    "    return [v for v in vendors if task_category in v.get(\"category_expertise\", [])]\n",
    "\n",
    "def format_vendors_for_prompt(vendors):\n",
    "    if not vendors:\n",
    "        return \"No suitable vendors found for this category or criteria.\"\n",
    "    # Include details relevant for selection\n",
    "    return \"\\n\".join([\n",
    "        f\"- ID: {v['vendor_id']}, Name: {v['name']}, Expertise: {v['category_expertise']}, \"\n",
    "        f\"Reliability: {v['past_reliability_score']}, Response Time (hrs): {v['response_time_hours']}, Constraints: {v['constraints']}\"\n",
    "        for v in vendors\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: dict\n",
    "    available_vendors: List[dict]\n",
    "    selected_vendor: Optional[dict]\n",
    "    rejected_vendors: List[str] \n",
    "    message: str              \n",
    "    error: Optional[str]         \n",
    "    max_retries: int          \n",
    "    retries_left: int         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_vendor(state: AgentState) -> AgentState:\n",
    "    \"\"\"Selects the best vendor based on task and available vendors, excluding rejected ones.\"\"\"\n",
    "    print(\"--- Node: select_vendor ---\")\n",
    "    task = state['task']\n",
    "    all_vendors = state['available_vendors']\n",
    "    rejected_ids = state['rejected_vendors']\n",
    "    retries_left = state['retries_left']\n",
    "\n",
    "    if retries_left <= 0:\n",
    "         print(\"Max retries reached.\")\n",
    "         return {\"error\": \"Max retries reached. Sending to human review.\"}\n",
    "\n",
    "    # 1. Filter by category\n",
    "    filtered_by_cat = filter_vendors_by_category(all_vendors, task)\n",
    "\n",
    "    # 2. Exclude rejected vendors\n",
    "    eligible_vendors = [v for v in filtered_by_cat if v['vendor_id'] not in rejected_ids]\n",
    "\n",
    "    if not eligible_vendors:\n",
    "        print(\"No eligible vendors found after filtering and exclusion.\")\n",
    "        return {\"error\": \"No eligible vendors left. Sending to human review.\"}\n",
    "\n",
    "    # 3. Prepare prompt for LLM\n",
    "    vendor_list_str = format_vendors_for_prompt(eligible_vendors)\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Task Description: {task_description}\n",
    "        Urgency: {urgency}\n",
    "        Special Requirements: {special_requirements}\n",
    "\n",
    "        Eligible Vendors (filtered by category '{task_category}' and excluding previously rejected):\n",
    "        {vendor_list_str}\n",
    "\n",
    "        Instructions: Analyze the task and the eligible vendors. Consider expertise, reliability, response time, constraints, and task urgency/requirements.\n",
    "        Select the single best vendor ID for this task. If multiple are equally good, pick one. If none seem suitable, output 'None'.\n",
    "        Explain your reasoning briefly and output the chosen vendor ID in the format: Selected Vendor ID: VXXX or Selected Vendor ID: None\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        task_description=task['task_description'],\n",
    "        urgency=task['urgency'],\n",
    "        special_requirements=task['special_requirements'],\n",
    "        task_category=task.get('category', 'N/A'),\n",
    "        vendor_list_str=vendor_list_str\n",
    "    )\n",
    "\n",
    "    # 4. Call LLM\n",
    "    try:\n",
    "        response_compete = llm.invoke(prompt) # Assuming llm is initialized ChatGoogleGenerativeAI\n",
    "        print(f\"LLM response complete: {response_compete}\")\n",
    "        response = response_compete.content\n",
    "        print(f\"LLM Selection Response: {response}\")\n",
    "\n",
    "        # 5. Parse LLM Response (Robust parsing needed here!)\n",
    "        # Example simple parsing (adapt based on actual LLM output format):\n",
    "        selected_id = None\n",
    "        if \"Selected Vendor ID:\" in response:\n",
    "            print(f\"Parsing LLM response for selected vendor ID: ${response.split('Selected Vendor ID:')}\")\n",
    "            potential_id = response.split(\"Selected Vendor ID:\")[-1].strip()\n",
    "            if potential_id.startswith(\"V\") and potential_id != \"None\":\n",
    "                 selected_id = potential_id\n",
    "\n",
    "        if selected_id:\n",
    "            selected_vendor_details = next((v for v in eligible_vendors if v['vendor_id'] == selected_id), None)\n",
    "            if selected_vendor_details:\n",
    "                print(f\"Selected Vendor: {selected_id}\")\n",
    "                return {\"selected_vendor\": selected_vendor_details, \"error\": None} # Successfully selected\n",
    "            else:\n",
    "                 print(f\"Error: LLM selected ID {selected_id} not found in eligible list.\")\n",
    "                 # Maybe retry selection or error out\n",
    "                 return {\"error\": f\"LLM selected invalid vendor ID {selected_id}\"}\n",
    "        else:\n",
    "            print(\"LLM did not select a vendor ('None' or parsing failed).\")\n",
    "            return {\"error\": \"LLM could not select a suitable vendor.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during vendor selection LLM call: {e}\")\n",
    "        return {\"error\": f\"LLM call failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_communication(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generates communication message for the selected vendor.\"\"\"\n",
    "    print(\"--- Node: generate_communication ---\")\n",
    "    task = state['task']\n",
    "    vendor = state['selected_vendor']\n",
    "\n",
    "    if not vendor:\n",
    "         # Should not happen if graph logic is correct, but good practice to check\n",
    "         print(\"Error: generate_communication called without a selected vendor.\")\n",
    "         return {\"message\": \"\", \"error\": \"Cannot generate message without selected vendor.\"}\n",
    "\n",
    "    # 1. Prepare prompt\n",
    "    comm_prompt = ChatPromptTemplate.from_template(\n",
    "       \"\"\"\n",
    "        Generate a message body to send to the vendor '{vendor_name}' about a new task assignment.\n",
    "\n",
    "        Vendor Details:\n",
    "        - Preferred Communication Style: {preferred_communication_style}\n",
    "\n",
    "        Task Details:\n",
    "        - Description: {task_description}\n",
    "        - Urgency: {urgency}\n",
    "        - Special Requirements: {special_requirements}\n",
    "\n",
    "        Instructions:\n",
    "        - Write the message in a {preferred_communication_style} tone.\n",
    "        - Include all relevant task details clearly.\n",
    "        - Make the message action-oriented with clear next steps (e.g., 'Please reply to confirm you can take this task' or 'Let us know your availability').\n",
    "        - Generate only the message body, without greetings like 'Hi...' or closings like 'Thanks,...'.\n",
    "        \"\"\"\n",
    "    ).format(\n",
    "        vendor_name=vendor['name'],\n",
    "        preferred_communication_style=vendor['preferred_communication_style'],\n",
    "        task_description=task['task_description'],\n",
    "        urgency=task['urgency'],\n",
    "        special_requirements=task['special_requirements']\n",
    "    )\n",
    "\n",
    "    # 2. Call LLM\n",
    "    try:\n",
    "        message_body = llm.invoke(comm_prompt) # Assuming llm is initialized\n",
    "        print(f\"Generated Message: {message_body}\")\n",
    "        return {\"message\": message_body, \"error\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during communication generation LLM call: {e}\")\n",
    "        return {\"message\": \"\", \"error\": f\"LLM call failed: {str(e)}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_vendor_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Simulates vendor accepting or rejecting the task.\"\"\"\n",
    "    print(\"--- Node: simulate_vendor_response ---\")\n",
    "    vendor = state['selected_vendor']\n",
    "    task = state['task']\n",
    "    if not vendor:\n",
    "         return {\"error\": \"Cannot simulate response without a selected vendor.\"} # Should not happen\n",
    "\n",
    "    # --- Add your simulation logic here ---\n",
    "    # Example: Vendor V003 rejects urgent tasks, otherwise 50/50 chance\n",
    "    rejected = False\n",
    "    if vendor['vendor_id'] == 'V003' and task['urgency'] == 'high':\n",
    "        rejected = True\n",
    "        print(f\"Simulating REJECTION for {vendor['vendor_id']} (Urgent task for V003)\")\n",
    "    elif random.random() < 0.3: # 30% chance of random rejection\n",
    "         rejected = True\n",
    "         print(f\"Simulating REJECTION for {vendor['vendor_id']} (Random chance)\")\n",
    "    else:\n",
    "        print(f\"Simulating ACCEPTANCE for {vendor['vendor_id']}\")\n",
    "\n",
    "    if rejected:\n",
    "        current_rejected = state.get('rejected_vendors', [])\n",
    "        new_rejected_list = current_rejected + [vendor['vendor_id']]\n",
    "        # Decrement retries for the next attempt\n",
    "        new_retries_left = state['retries_left'] - 1\n",
    "        return {\n",
    "            \"rejected_vendors\": new_rejected_list,\n",
    "            \"selected_vendor\": None, # Clear current vendor as they rejected\n",
    "            \"message\": \"\",           # Clear message\n",
    "            \"retries_left\": new_retries_left,\n",
    "            \"error\": \"Vendor rejected\" # Signal rejection\n",
    "        }\n",
    "    else:\n",
    "        # Vendor accepted! Keep selected_vendor and message. Clear error.\n",
    "        return {\"error\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determines the next step based on state.\"\"\"\n",
    "    print(\"--- Conditional Edge: should_continue ---\")\n",
    "    error = state.get('error')\n",
    "    retries_left = state['retries_left']\n",
    "\n",
    "    if error == \"Vendor rejected\":\n",
    "        print(f\"Decision: Vendor rejected, {retries_left} retries left.\")\n",
    "        if retries_left > 0:\n",
    "            return \"retry_select_vendor\" # Route back to select_vendor\n",
    "        else:\n",
    "            print(\"Decision: Max retries reached.\")\n",
    "            return \"human_review\" # Route to end state for human review\n",
    "    elif error:\n",
    "         # Any other error (no vendors found, LLM failed, etc.)\n",
    "         print(f\"Decision: Error encountered - {error}. Routing to human review.\")\n",
    "         return \"human_review\" # Route to end state for human review\n",
    "    else:\n",
    "         # No error means vendor presumably accepted in the simulation\n",
    "         print(\"Decision: Vendor accepted/no error.\")\n",
    "         return \"end_process\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"select_vendor\", select_vendor)\n",
    "workflow.add_node(\"generate_communication\", generate_communication)\n",
    "workflow.add_node(\"simulate_response\", simulate_vendor_response)\n",
    "workflow.add_node(\"human_review_node\", lambda state: print(\"--- Task Sent for Human Review --- \\nState:\", state) or {\"error\": \"Requires Human Intervention\"}) # Simple end node\n",
    "\n",
    "# Define edges\n",
    "workflow.set_entry_point(\"select_vendor\")\n",
    "workflow.add_edge(\"select_vendor\", \"generate_communication\")\n",
    "workflow.add_edge(\"generate_communication\", \"simulate_response\")\n",
    "\n",
    "# Conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"simulate_response\", # Source node\n",
    "    should_continue,     # Function to decide the route\n",
    "    {\n",
    "        \"retry_select_vendor\": \"select_vendor\", # If retry, go back to select\n",
    "        \"human_review\": \"human_review_node\",    # If error/max retries, go to human review\n",
    "        \"end_process\": END                      # If accepted, end the graph\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add an edge from the human_review node to END\n",
    "workflow.add_edge(\"human_review_node\", END)\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Graph ---\n",
      "--- Node: select_vendor ---\n",
      "LLM response complete: content=\"The task is urgent due to the sparking power outlet, prioritizing safety. The vendor 'Expert Electricians' has the required expertise ('electrical', 'repairs') and a good reliability score (4.6). However, their constraints pose a problem: they require a clear description of the issue beforehand (which we have provided), and they do not work on weekends (which could be a problem depending on the current day). The prompt doesn't provide the current day. Assuming it's a weekday, their response time of 3 hours, while not ideal for an emergency, is acceptable given their expertise and reliability.\\n\\nSelected Vendor ID: V005\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-001', 'safety_ratings': []} id='run-790757ec-67ac-427e-a8f6-7ce79ecfec47-0' usage_metadata={'input_tokens': 205, 'output_tokens': 134, 'total_tokens': 339, 'input_token_details': {'cache_read': 0}}\n",
      "LLM Selection Response: The task is urgent due to the sparking power outlet, prioritizing safety. The vendor 'Expert Electricians' has the required expertise ('electrical', 'repairs') and a good reliability score (4.6). However, their constraints pose a problem: they require a clear description of the issue beforehand (which we have provided), and they do not work on weekends (which could be a problem depending on the current day). The prompt doesn't provide the current day. Assuming it's a weekday, their response time of 3 hours, while not ideal for an emergency, is acceptable given their expertise and reliability.\n",
      "\n",
      "Selected Vendor ID: V005\n",
      "Parsing LLM response for selected vendor ID: $[\"The task is urgent due to the sparking power outlet, prioritizing safety. The vendor 'Expert Electricians' has the required expertise ('electrical', 'repairs') and a good reliability score (4.6). However, their constraints pose a problem: they require a clear description of the issue beforehand (which we have provided), and they do not work on weekends (which could be a problem depending on the current day). The prompt doesn't provide the current day. Assuming it's a weekday, their response time of 3 hours, while not ideal for an emergency, is acceptable given their expertise and reliability.\\n\\n\", ' V005']\n",
      "Selected Vendor: V005\n",
      "--- Node: generate_communication ---\n",
      "Generated Message: content='We are writing to request your immediate attention to a high-urgency task. A power outlet in the living room is sparking and requires the prompt attendance of a qualified electrician.\\n\\nSafety is of paramount importance. Therefore, we require that a certified professional be dispatched to address this issue.\\n\\nPlease reply to confirm your ability to undertake this task and provide details regarding your earliest availability to attend the property.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-001', 'safety_ratings': []} id='run-9b1bcc28-4f84-4ec7-8d7b-65a756d7fa02-0' usage_metadata={'input_tokens': 173, 'output_tokens': 80, 'total_tokens': 253, 'input_token_details': {'cache_read': 0}}\n",
      "--- Node: simulate_vendor_response ---\n",
      "Simulating ACCEPTANCE for V005\n",
      "--- Conditional Edge: should_continue ---\n",
      "Decision: Vendor accepted/no error.\n",
      "\n",
      "--- Graph Execution Finished ---\n",
      "Final State:\n",
      "{'category_expertise': ['repairs', 'electrical'],\n",
      " 'constraints': ['Requires clear description of issue beforehand',\n",
      "                 'Does not work on weekends'],\n",
      " 'name': 'Expert Electricians',\n",
      " 'past_reliability_score': 4.6,\n",
      " 'preferred_communication_style': 'formal',\n",
      " 'response_time_hours': 3,\n",
      " 'vendor_id': 'V005'}\n"
     ]
    }
   ],
   "source": [
    "task_to_process = tasks[4] # The urgent plumbing task\n",
    "\n",
    "# Initial state for the graph\n",
    "initial_state = AgentState(\n",
    "    task=task_to_process,\n",
    "    available_vendors=vendors,\n",
    "    selected_vendor=None,\n",
    "    rejected_vendors=[],\n",
    "    message=\"\",\n",
    "    error=None,\n",
    "    max_retries=3, # Allow up to 3 attempts\n",
    "    retries_left=3\n",
    ")\n",
    "\n",
    "# Invoke the graph\n",
    "print(\"\\n--- Running Graph ---\")\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- Graph Execution Finished ---\")\n",
    "print(\"Final State:\")\n",
    "\n",
    "# Pretty print the final state\n",
    "import pprint\n",
    "pprint.pprint(final_state['selected_vendor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
